{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1530022,"sourceType":"datasetVersion","datasetId":902117}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:00.202149Z","iopub.execute_input":"2025-04-07T12:27:00.202521Z","iopub.status.idle":"2025-04-07T12:27:00.219437Z","shell.execute_reply.started":"2025-04-07T12:27:00.202493Z","shell.execute_reply":"2025-04-07T12:27:00.218342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Downloading Dataset\n\nIn this section, we use the `kagglehub` library to download the latest version of the \"House Price Prediction Challenge\" dataset. This library allows us to directly download datasets from Kaggle without needing to manually upload files to the notebook.\n\n- `kagglehub.dataset_download()`: This function downloads the dataset specified by the Kaggle dataset identifier `\"anmolkumar/house-price-prediction-challenge\"`.\n- After the download, the dataset's path is printed to confirm where the files have been stored locally in the notebook environment.","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"anmolkumar/house-price-prediction-challenge\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:00.221151Z","iopub.execute_input":"2025-04-07T12:27:00.221554Z","iopub.status.idle":"2025-04-07T12:27:00.386548Z","shell.execute_reply.started":"2025-04-07T12:27:00.221504Z","shell.execute_reply":"2025-04-07T12:27:00.385652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Required Libraries\n\nIn this section, we import several Python libraries and modules that are essential for data manipulation, visualization, machine learning, and evaluation.\n\n- `pandas`: Used for data manipulation and analysis, especially for handling DataFrame objects.\n- `numpy`: Provides support for large, multi-dimensional arrays and matrices, as well as mathematical functions to operate on these arrays.\n- `seaborn`: A statistical data visualization library based on `matplotlib` that provides a high-level interface for drawing attractive and informative statistical graphics.\n- `matplotlib.pyplot`: A plotting library used for creating static, animated, and interactive visualizations in Python.\n- `plotly.express`: A library used to create interactive visualizations.\n- `scipy.stats`: Contains a large number of statistical functions for hypothesis testing, distributions, and more.\n- `joblib`: Used for saving (dump) and loading models (load), which is essential for model persistence and sharing.\n- `sklearn`: A collection of machine learning algorithms and tools for model building, preprocessing, and evaluation. Specifically:\n    - **Impute**: For handling missing data.\n    - **Pipeline**: To build end-to-end machine learning workflows.\n    - **ColumnTransformer**: For applying transformations to specific subsets of features.\n    - **RobustScaler**: A scaler that removes the median and scales the data according to the interquartile range (useful for robust machine learning).\n    - **OneHotEncoder**: A method for encoding categorical features.\n    - **DummyRegressor**: A baseline model for regression that predicts the mean of the target variable.\n    - **RandomForestRegressor**: A machine learning model for regression that uses an ensemble of decision trees.\n    - **GridSearchCV**: For hyperparameter tuning using cross-validation.\n    - **cross_val_score**: For evaluating model performance using cross-validation.\n    - **train_test_split**: To split the dataset into training and testing sets.\n    - **mean_squared_error** and **r2_score**: Common evaluation metrics for regression models.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport scipy.stats as stats\n\nfrom joblib import dump\nfrom joblib import load\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler, OneHotEncoder\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:50:54.311070Z","iopub.execute_input":"2025-04-07T13:50:54.311402Z","iopub.status.idle":"2025-04-07T13:50:54.317190Z","shell.execute_reply.started":"2025-04-07T13:50:54.311377Z","shell.execute_reply":"2025-04-07T13:50:54.316097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reducing Memory Usage\n\nIn this section, we define a function `reduce_memory_usage` that aims to optimize memory usage by downcasting numeric data types. This is particularly useful when working with large datasets, as it helps reduce the memory footprint by converting columns to smaller data types (e.g., `int64` to `int32` or `float64` to `float32`).\n\n- **Downcasting**: By converting columns to smaller data types, we can significantly reduce the memory consumption of our DataFrame without losing much precision.\n","metadata":{}},{"cell_type":"code","source":"# Reduce memory usage by downcasting numeric types\ndef reduce_memory_usage(df):\n    for col in df.select_dtypes(include=['int', 'float']).columns:\n        col_min = df[col].min()\n        col_max = df[col].max()\n        if pd.api.types.is_integer_dtype(df[col]):\n            df[col] = pd.to_numeric(df[col], downcast='integer')\n        else:\n            df[col] = pd.to_numeric(df[col], downcast='float')\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:00.395367Z","iopub.execute_input":"2025-04-07T12:27:00.395668Z","iopub.status.idle":"2025-04-07T12:27:00.414032Z","shell.execute_reply.started":"2025-04-07T12:27:00.395641Z","shell.execute_reply":"2025-04-07T12:27:00.412975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wrangle Function\n\nIn this section, we define the `wrangle` function, which is responsible for loading, cleaning, and transforming the dataset before any analysis or modeling. This function handles several preprocessing steps to ensure the dataset is ready for further analysis and modeling tasks.\n\nHereâ€™s a breakdown of the key steps:\n","metadata":{}},{"cell_type":"code","source":"# Define the 'wrangle' function to load and clean the data\ndef wrangle(filepath):\n    try:\n        # Read the CSV file into a DataFrame, handling missing values as 'NA'\n        df = pd.read_csv(filepath, na_values=['', 'NA', 'NaN'])\n    except FileNotFoundError as e:\n        print(f\"Error: File not found -> {e.filename}\")\n        return pd.DataFrame()\n\n    # Mask missing values (NaN) with pandas' native NA\n    df = df.mask(df.isna(), pd.NA)\n\n    # Output the basic shape and summary statistics of the data\n    print(f\"Shape: {df.shape}\")\n    print(df.describe())\n    print(\"\\nMissing values (% per column):\")\n    print(df.isna().mean().mul(100).sort_values(ascending=False).loc[lambda x: x > 0])\n\n    # Identify numeric and categorical columns\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    cat_cols = df.select_dtypes(include='object').columns\n\n    # Impute missing numeric values with the median if missing percentage is below 20%\n    for col in num_cols:\n        missing_pct = df[col].isna().mean()\n        if 0 < missing_pct < 0.2:\n            imputer = SimpleImputer(strategy='median')\n            df[col] = imputer.fit_transform(df[[col]])\n\n    # Impute missing categorical values with 'Unknown' if missing percentage is above 20%\n    for col in cat_cols:\n        missing_pct = df[col].isna().mean()\n        if missing_pct > 0.2:\n            df[col] = df[col].fillna(\"Unknown\")\n\n    # Create a new feature 'POSTED_BY_GROUPED' to group 'POSTED_BY' column values\n    if 'POSTED_BY' in df.columns:\n        df['POSTED_BY_GROUPED'] = df['POSTED_BY'].apply(lambda x: 'Dealer' if x == 'Dealer' else 'Non-Dealer')\n        df.drop(columns=['POSTED_BY'], inplace=True)\n\n    # Handle outliers in the key features 'TARGET(PRICE_IN_LACS)' and 'SQUARE_FT' using IQR\n    key_features = ['TARGET(PRICE_IN_LACS)', 'SQUARE_FT']\n    for feature in key_features:\n        if feature in df.columns:\n            Q1 = df[feature].quantile(0.25)\n            Q3 = df[feature].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            df = df[df[feature].between(lower_bound, upper_bound)]\n\n    # Create a new feature 'Price_per_sqft' to represent price per square foot\n    if 'TARGET(PRICE_IN_LACS)' in df.columns and 'SQUARE_FT' in df.columns:\n        df['Price_per_sqft'] = df['TARGET(PRICE_IN_LACS)'] / df['SQUARE_FT']\n\n    # Drop unnecessary columns like \"UNDER_CONSTRUCTION\", \"RESALE\", etc.\n    for col in [\"UNDER_CONSTRUCTION\", \"RESALE\", \"RERA\",\"Price_per_sqft\", \"BHK_OR_RK\"]:\n        if col in df.columns:\n            df.drop(columns=[col], inplace=True)\n\n    # Split the 'ADDRESS' column to extract the last part (city or region)\n    df[\"address\"] = df[\"ADDRESS\"].str.split(\",\").str[-1].str.strip()  \n    df.drop(columns=[\"ADDRESS\"], inplace=True)\n\n    # Reduce memory usage by downcasting numeric columns\n    df = reduce_memory_usage(df)\n\n    # Assert there are no missing values left\n    assert df.isna().sum().sum() == 0, \"There are still missing values in the dataset.\"\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:10:07.585368Z","iopub.execute_input":"2025-04-07T14:10:07.585756Z","iopub.status.idle":"2025-04-07T14:10:07.598097Z","shell.execute_reply.started":"2025-04-07T14:10:07.585727Z","shell.execute_reply":"2025-04-07T14:10:07.596732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wrangle Data and Preview the First Few Rows\n\nIn this step, we call the `wrangle` function on the training dataset located at the specified path. The function will process the data and return a cleaned DataFrame. Once the wrangling is complete, we preview the first few rows of the cleaned dataset using `df.head()` to verify that the data has been cleaned and transformed as expected.\n\n### Code to Wrangle and Preview:\n","metadata":{}},{"cell_type":"code","source":"# Wrangle the dataset and preview the first few rows\ndf = wrangle(\"/kaggle/input/house-price-prediction-challenge/train.csv\")\n\n# Display the first few rows of the cleaned dataset\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:11:49.959451Z","iopub.execute_input":"2025-04-07T14:11:49.959789Z","iopub.status.idle":"2025-04-07T14:11:50.506346Z","shell.execute_reply.started":"2025-04-07T14:11:49.959764Z","shell.execute_reply":"2025-04-07T14:11:50.505139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:00.637975Z","iopub.execute_input":"2025-04-07T12:27:00.638343Z","iopub.status.idle":"2025-04-07T12:27:00.645474Z","shell.execute_reply.started":"2025-04-07T12:27:00.638317Z","shell.execute_reply":"2025-04-07T12:27:00.644405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"BHK_OR_RK\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"POSTED_BY_GROUPED\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:16.870191Z","iopub.execute_input":"2025-04-07T12:27:16.870542Z","iopub.status.idle":"2025-04-07T12:27:16.882149Z","shell.execute_reply.started":"2025-04-07T12:27:16.870515Z","shell.execute_reply":"2025-04-07T12:27:16.880943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Class Balance for \"BHK_OR_RK\"\n\nIn this step, we plot a bar chart to examine the distribution of property types in the \"BHK_OR_RK\" column. The purpose of this visualization is to check for class balance or imbalance in the dataset.\n\n1. **Value Counts**: We use the `value_counts(normalize=True)` method to calculate the frequency distribution of the values in the \"BHK_OR_RK\" column. The `normalize=True` parameter scales the counts to show the relative frequencies (percentages) rather than the absolute counts.\n\n2. **Bar Plot**: We then plot the class balance using a bar chart (`kind=\"bar\"`) with labels for the x-axis (\"Type of property\") and y-axis (\"Frequency\"). The title \"Class Balance\" is added to describe the purpose of the chart.\n\nThis bar chart will help us understand how the data is distributed between the different types of properties and if any preprocessing (like dealing with class imbalance) is necessary.\n","metadata":{}},{"cell_type":"code","source":"df[\"BHK_OR_RK\"].value_counts(normalize=True).plot(\n    kind = \"bar\",\n    xlabel = \"Type of property\",\n    ylabel = \"Frequency\",\n    title = \"Class Balance\"\n);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the Distribution of \"TARGET(PRICE_IN_LACS)\"\n\nIn this step, we plot the distribution of the target variable, \"TARGET(PRICE_IN_LACS),\" which likely represents the house prices in lakhs. Understanding the distribution of the target variable is essential to decide the modeling techniques and potential data transformations.\n\n1. **Histogram with KDE**: We use `sns.histplot` to plot the distribution of the target variable. The histogram shows the frequency distribution of house prices, while the kernel density estimate (KDE) curve provides a smooth estimation of the probability density function.\n\n2. **Bins and Size**: We set the number of bins to 30 for better granularity and adjust the figure size to `(10, 5)` for better readability.\n\n3. **Title and Labels**: The plot title is set to \"TARGET(PRICE_IN_LACS) Distribution with KDE\" to describe the visualization. The x-axis is labeled as \"TARGET(PRICE_IN_LACS)\" to indicate the variable being plotted.\n\nThis visualization helps us understand the spread of house prices and whether the data is skewed or follows a certain distribution, such as normal or log-normal. It also helps in identifying outliers or extreme values in the target variable.\n","metadata":{}},{"cell_type":"code","source":"# Distribution of TARGET(PRICE_IN_LACS)\nplt.figure(figsize=(10, 5))\nsns.histplot(df['TARGET(PRICE_IN_LACS)'], kde=True, bins=30)\nplt.title(\"TARGET(PRICE_IN_LACS) Distribution with KDE\")\nplt.xlabel(\"TARGET(PRICE_IN_LACS)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:24.416424Z","iopub.execute_input":"2025-04-07T12:27:24.416751Z","iopub.status.idle":"2025-04-07T12:27:24.815641Z","shell.execute_reply.started":"2025-04-07T12:27:24.416725Z","shell.execute_reply":"2025-04-07T12:27:24.814572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the Mean Price by Address\n\nIn this section, we calculate and visualize the average house price for each unique address (from the `address` column) in the dataset. This can give us an understanding of the pricing trend across different locations. For the visualization, we sort the addresses based on the mean price and then plot a line graph.\n\n1. **Calculate Mean Price per Address**: We group the data by the \"address\" column and compute the mean of the \"TARGET(PRICE_IN_LACS)\" for each address. The result is sorted in ascending order to identify which locations have higher or lower average house prices.\n\n2. **Line Graph**: The line graph is plotted using the calculated mean prices, with the x-axis representing the sorted addresses (by index) and the y-axis representing the mean price. Markers are added to each data point for clarity.\n\n3. **Top and Bottom Cities**: We display the top 5 addresses with the highest mean prices and the bottom 5 addresses with the lowest mean prices for a quick overview.\n\nThis visualization helps us to identify locations where house prices are significantly higher or lower than the average. It also assists in detecting any trends or patterns related to geographical areas and house pricing.\n","metadata":{}},{"cell_type":"code","source":"# Calculate mean PRICE per ADDRESS_1 and sort\naddress1_price = df.groupby('address')['TARGET(PRICE_IN_LACS)'].mean().sort_values().reset_index()\n\n# Line graph\nplt.figure(figsize=(12, 6))\nplt.plot(address1_price.index, address1_price['TARGET(PRICE_IN_LACS)'], marker='o', linestyle='-', color='b')\nplt.title('Mean PRICE by address (Sorted)')\nplt.xlabel('ADDRESS Index (Sorted by Price)')\nplt.ylabel('Mean TARGET(PRICE_IN_LACS)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n# Top and bottom 5 for reference\nprint(\"Top 5 Cities by Mean PRICE:\")\nprint(address1_price.tail(10))\nprint(\"\\nBottom 5 Cities by Mean PRICE:\")\nprint(address1_price.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:27:49.922919Z","iopub.execute_input":"2025-04-07T12:27:49.923309Z","iopub.status.idle":"2025-04-07T12:27:50.173011Z","shell.execute_reply.started":"2025-04-07T12:27:49.923279Z","shell.execute_reply":"2025-04-07T12:27:50.171843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation Heatmap\n\nIn this section, we analyze the relationships between different numerical features and their correlation with the target variable, `TARGET(PRICE_IN_LACS)`.\n\n1. **Correlation Calculation**: We compute the correlation matrix for the numerical features in the dataset. Correlation measures the strength and direction of the linear relationship between two variables, with values ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation).\n\n2. **Select Top Correlated Features**: We specifically focus on the features that have the highest absolute correlation with the target variable (`TARGET(PRICE_IN_LACS)`). These are the features most likely to influence the price.\n\n3. **Heatmap Visualization**: The heatmap is then generated to visualize the correlation matrix for the top correlated features. The intensity of the colors (from blue to red) represents the strength of the correlation, with blue indicating negative correlation and red indicating positive correlation.\n\n4. **Key Insights**: By visualizing these relationships, we can identify which features have the strongest influence on house prices. This can help in building a better predictive model by focusing on the most important features.\n\n### Key Takeaways:\n- The heatmap provides a clear understanding of which features correlate strongly with `TARGET(PRICE_IN_LACS)`.\n- Features that show a high correlation with the target are often the most influential for predicting house prices, and thus should be prioritized in model development.\n","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap\nplt.figure(figsize=(12, 10))\ncorr = df.corr(numeric_only=True)\ntop_corr = corr['TARGET(PRICE_IN_LACS)'].abs().sort_values(ascending=False).head().index\nsns.heatmap(df[top_corr].corr(), annot=True, cmap='coolwarm')\nplt.title(\"Top Correlated Features with TARGET(PRICE_IN_LACS)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:21:09.670426Z","iopub.execute_input":"2025-04-07T14:21:09.670771Z","iopub.status.idle":"2025-04-07T14:21:10.017214Z","shell.execute_reply.started":"2025-04-07T14:21:09.670746Z","shell.execute_reply":"2025-04-07T14:21:10.016012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Statistical Test Results\n\nIn this section, we perform statistical tests to assess the relationships between different features and the target variable, `TARGET(PRICE_IN_LACS)`.\n\n1. **ANOVA (Analysis of Variance)**: \n   - We apply ANOVA to categorical features with more than one unique value to determine whether the mean `TARGET(PRICE_IN_LACS)` differs significantly across the categories. A low p-value (typically < 0.05) indicates that there is a significant difference in the mean target variable across the categories.\n   \n2. **Pearson's Correlation**: \n   - Pearson's test measures the linear correlation between two variables. A low p-value (< 0.05) indicates that there is a statistically significant linear relationship between the numeric feature and the target variable.\n\n3. **Spearman's Rank Correlation**: \n   - Spearman's correlation assesses the monotonic relationship between two variables, which can capture non-linear but still ordered relationships. Like Pearson's, a low p-value suggests a significant correlation.\n\n4. **Sorting the Results**: \n   - After performing the tests, the results are organized in a table and sorted by the p-value, with the most statistically significant results appearing at the top.\n\n### Key Observations:\n- **Significant Categorical Variables**: Features like `POSTED_BY_GROUPED` and `address` show very low p-values (close to 0), indicating that they have a significant influence on the target variable.\n  \n- **Significant Numeric Variables**: Features such as `BHK_NO.` and `SQUARE_FT` have p-values of 0, showing a strong linear relationship with the target variable. Additionally, geographical features like `LATITUDE` and `LONGITUDE` also have very low p-values, suggesting their significant impact on property prices.\n\nBy identifying the significant features through statistical tests, we can prioritize them when building predictive models.\n\n","metadata":{}},{"cell_type":"code","source":"# Statistical tests table\nanova_results = []\nnum_corr_results = []\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        if df[col].nunique() > 1:\n            grouped = df[[col, 'TARGET(PRICE_IN_LACS)']].dropna().groupby(col)\n            if len(grouped) > 1:\n                f_val, p_val = stats.f_oneway(*(grouped['TARGET(PRICE_IN_LACS)'].apply(list)))\n                anova_results.append({'Feature': col, 'Test': 'ANOVA', 'P-Value': p_val})\n    elif pd.api.types.is_numeric_dtype(df[col]) and col != 'TARGET(PRICE_IN_LACS)':\n        pearson = stats.pearsonr(df[col], df['TARGET(PRICE_IN_LACS)'])[1]\n        spearman = stats.spearmanr(df[col], df['TARGET(PRICE_IN_LACS)'])[1]\n        num_corr_results.append({'Feature': col, 'Test': 'Pearson', 'P-Value': pearson})\n        num_corr_results.append({'Feature': col, 'Test': 'Spearman', 'P-Value': spearman})\n\nstats_df = pd.DataFrame(anova_results + num_corr_results)\nprint(\"\\nStatistical Test Results:\")\nprint(stats_df.sort_values(\"P-Value\").head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:28:03.463374Z","iopub.execute_input":"2025-04-07T12:28:03.463823Z","iopub.status.idle":"2025-04-07T12:28:03.590229Z","shell.execute_reply.started":"2025-04-07T12:28:03.463785Z","shell.execute_reply":"2025-04-07T12:28:03.589132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature/Target Split\n\nIn this step, we separate the features (input variables) and the target (output variable) that we aim to predict. \n\n1. **Target Variable**:\n   - The target variable is the column `TARGET(PRICE_IN_LACS)`, which represents the property prices in the dataset.\n   - This is the variable we want to predict using the features (input variables).\n\n2. **Feature Variables**:\n   - All other columns except for `TARGET(PRICE_IN_LACS)` are considered feature variables.\n   - These are the independent variables that will be used to train machine learning models to predict the target.","metadata":{}},{"cell_type":"code","source":"# Feature/target split\ntarget = \"TARGET(PRICE_IN_LACS)\"\nX = df.drop(columns=[\"TARGET(PRICE_IN_LACS)\"])\ny = df[target]\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:28:17.494619Z","iopub.execute_input":"2025-04-07T12:28:17.495030Z","iopub.status.idle":"2025-04-07T12:28:17.504806Z","shell.execute_reply.started":"2025-04-07T12:28:17.494998Z","shell.execute_reply":"2025-04-07T12:28:17.503680Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train-Test Split\n\nIn this step, we split the dataset into training and testing sets. This is a common practice in machine learning to evaluate how well a model generalizes to unseen data.\n\n1. **Training Set**:\n   - The training set is used to train the model. It contains 80% of the data.\n   - The model learns the relationships between the features and the target during the training process.\n\n2. **Testing Set**:\n   - The testing set is used to evaluate the performance of the model after training. It contains 20% of the data.\n   - This set simulates unseen data to test how well the model can make predictions on new data.\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:28:19.877813Z","iopub.execute_input":"2025-04-07T12:28:19.878190Z","iopub.status.idle":"2025-04-07T12:28:19.896541Z","shell.execute_reply.started":"2025-04-07T12:28:19.878162Z","shell.execute_reply":"2025-04-07T12:28:19.895192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline Model (DummyRegressor)\n\nA **baseline model** serves as a simple reference point to evaluate the performance of more complex models. In this case, we use the **DummyRegressor** from Scikit-learn, which makes predictions based on simple strategies, such as predicting the mean of the target variable for all samples.\n\n### Strategy: \"mean\"\n- The **DummyRegressor** with `strategy=\"mean\"` predicts the mean value of the target variable (`TARGET(PRICE_IN_LACS)`) for all samples. It does not use any of the features to make predictions, but rather just outputs the average value of the target for the entire dataset.\n\n### RMSE Calculation\nThe **Root Mean Squared Error (RMSE)** is used to evaluate how well the baseline model is performing. It measures the average magnitude of the errors between the predicted and actual values. The lower the RMSE, the better the model's performance.","metadata":{}},{"cell_type":"code","source":"# Baseline model (DummyRegressor)\nbaseline = DummyRegressor(strategy=\"mean\")\nbaseline.fit(X_train, y_train)\ny_pred_baseline = baseline.predict(X_test)\nprint(\"Baseline RMSE:\", mean_squared_error(y_test, y_pred_baseline, squared=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:28:22.875632Z","iopub.execute_input":"2025-04-07T12:28:22.875991Z","iopub.status.idle":"2025-04-07T12:28:22.883820Z","shell.execute_reply.started":"2025-04-07T12:28:22.875965Z","shell.execute_reply":"2025-04-07T12:28:22.883013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing Pipeline\n\nThe **preprocessing pipeline** is essential to prepare the data for machine learning models. It ensures that the data is cleaned, scaled, and encoded in a way that can be easily ingested by algorithms. In this section, we create a pipeline that handles both **numerical** and **categorical** features.\n\n### Steps:\n1. **Numerical Features**:\n   - **Imputation**: For missing values in numerical columns, we use the `SimpleImputer` with the strategy set to `'median'`. This replaces any missing values in numerical columns with the median value of that column. The median is chosen as it is robust to outliers.\n   - **Scaling**: After imputation, we apply the `RobustScaler` to scale the numerical features. This scaler is less sensitive to outliers and scales the features based on the interquartile range (IQR), making it suitable for features with skewed distributions.\n\n2. **Categorical Features**:\n   - **Imputation**: For categorical columns with missing values, we use `SimpleImputer` with the strategy set to `'constant'` and the `fill_value` set to `'missing'`. This ensures that missing categorical values are filled with the string `'missing'`.\n   - **Encoding**: After imputation, we apply `OneHotEncoder` to transform categorical features into a binary matrix. The `handle_unknown='ignore'` parameter ensures that any new categories appearing in the test set (that were not present during training) will be ignored and encoded as all zeros.\n","metadata":{}},{"cell_type":"code","source":"# Preprocessing pipeline\nnum_features = X.select_dtypes(include=[np.number]).columns.tolist()\ncat_features = X.select_dtypes(include='object').columns.tolist()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', make_pipeline(SimpleImputer(strategy='median'), RobustScaler()), num_features),\n        ('cat', make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'), OneHotEncoder(handle_unknown='ignore')), cat_features)\n    ]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:29:16.844671Z","iopub.execute_input":"2025-04-07T12:29:16.845153Z","iopub.status.idle":"2025-04-07T12:29:16.858452Z","shell.execute_reply.started":"2025-04-07T12:29:16.845113Z","shell.execute_reply":"2025-04-07T12:29:16.857207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Pipeline and Hyperparameter Tuning\n\nIn this section, we define the machine learning model pipeline, perform cross-validation to assess model performance, and perform hyperparameter tuning to optimize the model.\n\n### 1. **Model Pipeline**:\nWe build a **Pipeline** consisting of two main steps:\n- **Preprocessing**: The preprocessing step uses the `preprocessor` we defined earlier. This handles the imputation, scaling, and encoding of the features in the dataset.\n- **RandomForestRegressor**: We use a `RandomForestRegressor`, which is an ensemble model that combines the predictions of many decision trees to improve accuracy and reduce overfitting. We set a `random_state` for reproducibility.\n\n### 2. **Cross-Validation**:\n- We use **cross-validation (CV)** to evaluate the model's performance. Cross-validation splits the training data into multiple subsets and trains the model on each subset while testing on the remaining data. The results are averaged to give a more robust estimate of the model's performance.\n- The evaluation metric we use is the **Root Mean Squared Error (RMSE)**, as it is a common metric for regression tasks. The negative sign is used because `cross_val_score` returns scores, and we want to minimize the RMSE.\n\n### 3. **Hyperparameter Tuning with GridSearchCV**:\n- **GridSearchCV** is used to perform an exhaustive search over a range of hyperparameters for the `RandomForestRegressor`.\n- We specify a set of hyperparameters to tune:\n  - **n_estimators**: The number of trees in the forest. We try different values from 25 to 100.\n  - **max_depth**: The maximum depth of the trees. We explore values from 10 to 50.\n- `GridSearchCV` will try all combinations of these hyperparameters and find the best set based on cross-validation performance.\n\n### Key Points:\n- **Cross-Validation** ensures the model is not overfitting or underfitting by evaluating it on multiple subsets of the data.\n- **Hyperparameter tuning** optimizes the modelâ€™s parameters for the best performance.\n","metadata":{}},{"cell_type":"code","source":"model = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', RandomForestRegressor(random_state=42))\n])\n\nprint(model)\n\ncv_rmse_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\nprint(\"CV RMSE scores:\", cv_rmse_scores)\n\nparams = {\n    \"regressor__n_estimators\": range(25, 100, 25),\n    \"regressor__max_depth\": range(10, 50, 10)\n}\n\ngs_model = GridSearchCV(\n    model,\n    param_grid=params,\n    cv=5,\n    n_jobs=-1,\n    verbose=1,\n    scoring='neg_root_mean_squared_error'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:29:22.356191Z","iopub.execute_input":"2025-04-07T12:29:22.356534Z","iopub.status.idle":"2025-04-07T12:31:48.135967Z","shell.execute_reply.started":"2025-04-07T12:29:22.356509Z","shell.execute_reply":"2025-04-07T12:31:48.134696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training and Results\n\nIn this section, we train the model using **GridSearchCV** and evaluate the results.\n\n### 1. **Model Fitting**:\n- We train the model on the training data (`X_train`, `y_train`) by fitting the `GridSearchCV` model. During this process, `GridSearchCV` trains the model for each combination of hyperparameters in the defined search space (the range of `n_estimators` and `max_depth`).\n\n### 2. **Best Parameters**:\n- After the grid search completes, we retrieve the best hyperparameters that resulted in the lowest cross-validated RMSE:\n  - **`max_depth`**: 20\n  - **`n_estimators`**: 75\n  \nThese are the optimal values for the model, selected from all the candidates.\n\n### 3. **Best Cross-Validated RMSE**:\n- The best RMSE score obtained from cross-validation is approximately **19.36**. This value indicates the root mean squared error of the model when evaluated on multiple folds during cross-validation. A lower RMSE means the model's predictions are closer to the actual values.\n\n### Conclusion:\n- **Best Parameters**: The model performs best with 75 trees (`n_estimators=75`) and a maximum tree depth of 20 (`max_depth=20`).\n- **Performance**: The optimized model achieved a best cross-validated RMSE of **19.36**, indicating a relatively good fit.\n\nThis result suggests that the model's performance can be improved with the chosen hyperparameters, but there may still be room for further optimization.\n","metadata":{}},{"cell_type":"code","source":"# Train model\ngs_model.fit(X_train, y_train)\n\nprint(\"Best parameters:\", gs_model.best_params_)\nprint(\"Best cross-validated RMSE:\", -gs_model.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:32:37.144344Z","iopub.execute_input":"2025-04-07T12:32:37.144714Z","iopub.status.idle":"2025-04-07T12:41:37.154824Z","shell.execute_reply.started":"2025-04-07T12:32:37.144684Z","shell.execute_reply":"2025-04-07T12:41:37.153758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation on Test Set\n\nAfter training the model with the optimal hyperparameters, we evaluate its performance on the test set.\n\n### 1. **Predictions on Test Set**:\n- We use the `gs_model` (which contains the best model found through GridSearchCV) to predict the target variable on the test set (`X_test`).\n\n### 2. **Evaluation Metrics**:\n- We compute the following key metrics to assess the model's performance on the test set:\n\n  - **Test RMSE** (Root Mean Squared Error): This metric measures the average difference between predicted and actual values. The RMSE for the test set is **19.55**, which indicates the model's error on unseen data.\n\n  - **Test RÂ² (R-squared)**: This metric tells us the proportion of the variance in the target variable that is explained by the model. The RÂ² value is **0.757**, which means that the model explains approximately **75.7%** of the variance in the target variable on the test set.\n\n### Conclusion:\n- **Test RMSE**: 19.55\n- **Test RÂ²**: 0.757\n\nThe model performs fairly well on the test set, with a moderate RMSE and a solid RÂ² value, showing that it generalizes well to unseen data. However, there is always room for improvement in model tuning, feature engineering, or using more advanced models.\n","metadata":{}},{"cell_type":"code","source":"# Evaluate on test set\ny_pred = gs_model.predict(X_test)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\ntest_r2 = r2_score(y_test, y_pred)\nprint(\"Test RMSE:\", test_rmse)\nprint(\"Test RÂ²:\", test_r2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:44:46.170608Z","iopub.execute_input":"2025-04-07T12:44:46.170977Z","iopub.status.idle":"2025-04-07T12:44:46.309702Z","shell.execute_reply.started":"2025-04-07T12:44:46.170950Z","shell.execute_reply":"2025-04-07T12:44:46.308637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Residuals Analysis\n\nResidual analysis helps to understand the difference between the actual and predicted values (i.e., the error made by the model). It provides insights into the model's performance and helps identify potential issues, such as heteroscedasticity or model bias.\n\n### 1. **Residual Calculation**:\n- Residuals are calculated as the difference between the actual target values (`y_test`) and the predicted values (`y_pred`) from the model. A positive residual means the model under-predicted, and a negative residual indicates over-prediction.\n\n```python\nresiduals = y_test - y_pred\n","metadata":{}},{"cell_type":"code","source":"# Residuals\nresiduals = y_test - y_pred\n\n# Residuals distribution plot\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nsns.histplot(residuals, kde=True, bins=30, color='tomato')\nplt.title('Residuals Distribution')\nplt.xlabel('Residual (Actual - Predicted)')\nplt.axvline(0, linestyle='--', color='black')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T12:48:28.875261Z","iopub.execute_input":"2025-04-07T12:48:28.875633Z","iopub.status.idle":"2025-04-07T12:48:29.238412Z","shell.execute_reply.started":"2025-04-07T12:48:28.875607Z","shell.execute_reply":"2025-04-07T12:48:29.237329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance from Random Forest\n\nFeature importance indicates how relevant each feature is in predicting the target variable. In a Random Forest model, feature importance is computed by looking at how much each feature contributes to reducing the impurity (e.g., Gini impurity or entropy) at each split in the trees.\n\n### 1. **Best Model Selection**:\n- The best model is selected from the grid search results, which provides the optimal parameters (`n_estimators` and `max_depth`) for the Random Forest regressor.\n```python\nbest_model = gs_model.best_estimator_\n","metadata":{}},{"cell_type":"code","source":"# Get the best estimator from GridSearchCV\nbest_model = gs_model.best_estimator_\n\n# Extract the RandomForestRegressor from the pipeline\nrf_model = best_model.named_steps['regressor']\n\n# Get feature importances\nimportances = rf_model.feature_importances_\n\n# Get feature names from the preprocessor\n# For numerical features (after RobustScaler)\nnum_features_transformed = num_features  # since RobustScaler doesn't change feature count\n\n# For categorical features (after OneHotEncoder)\ncat_pipeline = best_model.named_steps['preprocessor'].named_transformers_['cat']\nohe = cat_pipeline.named_steps['onehotencoder']\ncat_features_transformed = ohe.get_feature_names_out(cat_features)\n\n# Combine all feature names\nall_feature_names = np.concatenate([num_features_transformed, cat_features_transformed])\n\n# Create a DataFrame for feature importances\nfeature_importance_df = pd.DataFrame({\n    'Feature': all_feature_names,\n    'Importance': importances\n}).sort_values('Importance', ascending=False)\n\n# Plot feature importances\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20), palette='viridis')\nplt.title('Top 20 Feature Importances from Random Forest')\nplt.tight_layout()\nplt.show()\n\n# You can also print the feature importances\nprint(feature_importance_df.head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:38:24.942857Z","iopub.execute_input":"2025-04-07T13:38:24.943275Z","iopub.status.idle":"2025-04-07T13:38:25.798750Z","shell.execute_reply.started":"2025-04-07T13:38:24.943243Z","shell.execute_reply":"2025-04-07T13:38:25.797612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the Best Model\n\nOnce we have the best model identified through GridSearchCV, we save it for future use, avoiding the need to retrain the model each time.\n\n### 1. **Extracting the Best Model**:\nThe `best_estimator_` attribute from the `GridSearchCV` object contains the best performing model, which includes the optimal hyperparameters found during the grid search process. We extract this model.\n","metadata":{}},{"cell_type":"code","source":"# Save the best model from GridSearchCV\nbest_model = gs_model.best_estimator_\ndump(best_model, 'best_model.joblib')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:44:54.157045Z","iopub.execute_input":"2025-04-07T13:44:54.157370Z","iopub.status.idle":"2025-04-07T13:44:54.275339Z","shell.execute_reply.started":"2025-04-07T13:44:54.157346Z","shell.execute_reply":"2025-04-07T13:44:54.274329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Function: `make_predictions`\n\nThis function generates predictions on a given test dataset using a previously trained and serialized model. It takes in the path to the test data and the path to the serialized model, cleans the test data, loads the model, and then generates and returns the predictions.\n\n### Function Parameters:\n- **test_data_path** (str): \n  - The path to the test dataset (CSV file) that contains the data you want to make predictions on.\n  \n- **model_path** (str): \n  - The path to the serialized model file (e.g., `.joblib` or `.pkl`) that will be used for generating predictions.\n\n### Function Return:\n- **pd.Series**: \n  - A Pandas Series containing the predictions, indexed by the test data's index. The name of the Series is `'predictions'`","metadata":{}},{"cell_type":"code","source":"def make_predictions(test_data_path: str, model_path: str) -> pd.Series:\n    \"\"\"\n    Generate predictions using a serialized model on cleaned test data.\n    \n    Parameters:\n    -----------\n    test_data_path : str\n        Path to CSV file containing test data\n    model_path : str\n        Path to serialized model file (.joblib or .pkl)\n    \n    Returns:\n    --------\n    pd.Series\n        Series containing predictions\n    \"\"\"\n    # 1. Load and clean test data\n    test_data = wrangle(test_data_path)\n    \n    # 2. Load serialized model\n    model = load(model_path)\n    \n    # 3. Generate predictions\n    predictions = model.predict(test_data)\n    \n    # 4. Convert to Series with appropriate name\n    return pd.Series(predictions, name='predictions', index=test_data.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:51:14.765021Z","iopub.execute_input":"2025-04-07T13:51:14.765376Z","iopub.status.idle":"2025-04-07T13:51:14.771194Z","shell.execute_reply.started":"2025-04-07T13:51:14.765346Z","shell.execute_reply":"2025-04-07T13:51:14.769933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making Final Predictions on Test Data\n\nIn this step, we use the `make_predictions` function to generate predictions on the test dataset provided in the competition. We pass the file path of the test data and the path to the trained model file.\n","metadata":{}},{"cell_type":"code","source":"predictions = make_predictions('/kaggle/input/house-price-prediction-challenge/test.csv', 'best_model.joblib')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:52:42.497458Z","iopub.execute_input":"2025-04-07T13:52:42.498009Z","iopub.status.idle":"2025-04-07T13:52:44.436183Z","shell.execute_reply.started":"2025-04-07T13:52:42.497947Z","shell.execute_reply":"2025-04-07T13:52:44.435011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:58:59.579976Z","iopub.execute_input":"2025-04-07T14:58:59.580335Z","iopub.status.idle":"2025-04-07T14:58:59.694004Z","shell.execute_reply.started":"2025-04-07T14:58:59.580307Z","shell.execute_reply":"2025-04-07T14:58:59.692925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:01:35.587206Z","iopub.execute_input":"2025-04-07T15:01:35.587567Z","iopub.status.idle":"2025-04-07T15:01:35.595803Z","shell.execute_reply.started":"2025-04-07T15:01:35.587541Z","shell.execute_reply":"2025-04-07T15:01:35.594584Z"}},"outputs":[],"execution_count":null}]}